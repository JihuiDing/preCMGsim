{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b454c3",
   "metadata": {},
   "source": [
    "# importance sampling using mixture proposal distributions for two independent uniform variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05d5db",
   "metadata": {},
   "source": [
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e80b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample  1: SH_azi=309.83, SH=17.69, weight=0.0243\n",
      "Sample  2: SH_azi=311.50, SH=17.10, weight=0.0061\n",
      "Sample  3: SH_azi=311.69, SH=19.38, weight=0.0061\n",
      "Sample  4: SH_azi=313.92, SH=16.34, weight=0.0069\n",
      "Sample  5: SH_azi=311.24, SH=16.34, weight=0.0069\n",
      "Sample  6: SH_azi=313.79, SH=17.56, weight=0.0061\n",
      "Sample  7: SH_azi=316.03, SH=17.39, weight=0.0061\n",
      "Sample  8: SH_azi=310.82, SH=18.72, weight=0.0061\n",
      "Sample  9: SH_azi=304.29, SH=18.34, weight=0.0243\n",
      "Sample 10: SH_azi=303.70, SH=17.14, weight=0.0243\n",
      "\n",
      "Total samples: 100\n",
      "Sum of weights: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_samples = 100\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "\n",
    "# Target distributions\n",
    "target_SH_azi_low, target_SH_azi_high = 300, 320\n",
    "target_SH_low, target_SH_high = 16.2, 19.8\n",
    "\n",
    "# Proposal mixture components\n",
    "prop1_SH_azi = (310, 320)\n",
    "prop2_SH_azi = (300, 310)\n",
    "prop1_SH = (17, 19.8)\n",
    "prop2_SH = (16.2, 17)\n",
    "\n",
    "# ----- Sample from proposals -----\n",
    "n1_azi = int(alpha * n_samples)\n",
    "n2_azi = n_samples - n1_azi\n",
    "n1_SH = int(beta * n_samples)\n",
    "n2_SH = n_samples - n1_SH\n",
    "\n",
    "samples_SH_azi = np.concatenate([\n",
    "    np.random.uniform(*prop1_SH_azi, size=n1_azi),\n",
    "    np.random.uniform(*prop2_SH_azi, size=n2_azi)\n",
    "])\n",
    "\n",
    "samples_SH = np.concatenate([\n",
    "    np.random.uniform(*prop1_SH, size=n1_SH),\n",
    "    np.random.uniform(*prop2_SH, size=n2_SH)\n",
    "])\n",
    "\n",
    "# Shuffle to mix samples\n",
    "np.random.shuffle(samples_SH_azi)\n",
    "np.random.shuffle(samples_SH)\n",
    "\n",
    "# ----- Compute densities -----\n",
    "def uniform_pdf(x, low, high):\n",
    "    return np.where((x >= low) & (x <= high), 1.0 / (high - low), 0.0)\n",
    "\n",
    "def mixture_uniform_pdf(x, low1, high1, low2, high2, w):\n",
    "    pdf1 = np.where((x >= low1) & (x <= high1), 1.0 / (high1 - low1), 0.0)\n",
    "    pdf2 = np.where((x >= low2) & (x <= high2), 1.0 / (high2 - low2), 0.0)\n",
    "    return w * pdf1 + (1 - w) * pdf2\n",
    "\n",
    "target_pdf_SH_azi = uniform_pdf(samples_SH_azi, target_SH_azi_low, target_SH_azi_high)\n",
    "target_pdf_SH = uniform_pdf(samples_SH, target_SH_low, target_SH_high)\n",
    "\n",
    "proposal_pdf_SH_azi = mixture_uniform_pdf(samples_SH_azi, *prop1_SH_azi, *prop2_SH_azi, alpha)\n",
    "proposal_pdf_SH = mixture_uniform_pdf(samples_SH, *prop1_SH, *prop2_SH, beta)\n",
    "\n",
    "# ----- Importance weights -----\n",
    "weights = (target_pdf_SH_azi * target_pdf_SH) / (proposal_pdf_SH_azi * proposal_pdf_SH)\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "# ----- Example output -----\n",
    "for i in range(10):  # show first 10 samples\n",
    "    print(f\"Sample {i+1:2d}: SH_azi={samples_SH_azi[i]:.2f}, SH={samples_SH[i]:.2f}, weight={weights[i]:.4f}\")\n",
    "\n",
    "print(\"\\nTotal samples:\", len(samples_SH))\n",
    "print(\"Sum of weights:\", np.sum(weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fbc18",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b293b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH_azi samples min: 300.86\n",
      "SH_azi samples max: 319.89\n",
      "SH samples min: 16.26\n",
      "SH samples max: 19.73\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(11)\n",
    "name_prefix = '250922'\n",
    "# Parameters\n",
    "n_samples = 90\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "\n",
    "# --- Target distributions ---\n",
    "# SH_azi ~ U(300, 320)\n",
    "target_SH_azi_low, target_SH_azi_high = 300, 320\n",
    "p_SH_azi = 1/(target_SH_azi_high - target_SH_azi_low)\n",
    "\n",
    "# SH ~ U(16.2, 19.8)\n",
    "target_SH_low, target_SH_high = 16.2, 19.8\n",
    "p_SH = 1/(target_SH_high - target_SH_low)\n",
    "\n",
    "# --- Proposal mixture components ---\n",
    "# SH_azi: 0.2 U(300,310) + 0.8 U(310,320)\n",
    "u1 = np.random.rand(n_samples)\n",
    "u1_quantile = np.quantile(u1,alpha)\n",
    "samples_SH_azi = np.empty(n_samples)\n",
    "samples_SH_azi[u1 > u1_quantile] = np.random.uniform(300, 310, size=(u1 > u1_quantile).sum())\n",
    "samples_SH_azi[u1 <= u1_quantile] = np.random.uniform(310, 320, size=(u1 <= u1_quantile).sum())\n",
    "q_SH_azi = np.where((samples_SH_azi >= 310) & (samples_SH_azi < 320), alpha / 10.0, (1 - alpha) / 10.0)\n",
    "print(f'SH_azi samples min: {np.min(samples_SH_azi):.2f}')\n",
    "print(f'SH_azi samples max: {np.max(samples_SH_azi):.2f}')\n",
    "\n",
    "# SH: 0.2 U(16.2,17) + 0.8 U(17,19.8)\n",
    "u2 = np.random.rand(n_samples)\n",
    "u2_quantile = np.quantile(u2,beta)\n",
    "samples_SH = np.empty(n_samples)\n",
    "samples_SH[u2 > u2_quantile] = np.random.uniform(16.2, 17.0, size=(u2 > u2_quantile).sum())\n",
    "samples_SH[u2 <= u2_quantile] = np.random.uniform(17.0, 19.8, size=(u2 <= u2_quantile).sum())\n",
    "q_SH = np.where((samples_SH >= 17) & (samples_SH < 19.8), beta / (19.8 - 17.0), (1 - beta) / (17.0 - 16.2))\n",
    "print(f'SH samples min: {np.min(samples_SH):.2f}')\n",
    "print(f'SH samples max: {np.max(samples_SH):.2f}')\n",
    "\n",
    "# --- Importance weights ---\n",
    "# Since the two variables are independent, total weight = (p_SH_azi * p_SH) / (q_SH_azi*q_SH)\n",
    "weights = (p_SH_azi * p_SH) / (q_SH_azi * q_SH)\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "importance_sampling = np.column_stack((samples_SH_azi,q_SH_azi,samples_SH,q_SH,weights))\n",
    "\n",
    "header_string = 'SH_azi,q_SH_azi,SH,q_SH,weights'\n",
    "np.savetxt(f'data/{name_prefix}_importance_sampling.csv',importance_sampling,delimiter=',',fmt='%.4f',header=header_string,comments='')\n",
    "print(np.sum(importance_sampling[:,0]>310))\n",
    "print(np.sum(importance_sampling[:,2]>17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a018eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(importance_sampling[:,0]>310))\n",
    "print(np.sum(importance_sampling[:,2]>17))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
